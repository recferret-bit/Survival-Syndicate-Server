# Survival Syndicate: Гайд по разработке серверной части

Этот документ описывает ключевые принципы и лучшие практики, которых мы придерживаемся при разработке серверной архитектуры проекта.

## Часть 1: Философия

1.  **Простота > Сложность (Simple is better than complex)**
    -   Мы избегаем "магических" решений. Код должен быть читаемым и понятным даже для нового разработчика.
    -   Каждый микросервис решает одну, четко определенную задачу (Single Responsibility Principle).

2.  **Явное > Неявное (Explicit is better than implicit)**
    -   Все зависимости должны быть явно объявлены. Мы избегаем глобальных состояний.
    -   Контракты взаимодействия между сервисами (сообщения в NATS) должны быть строго определены через Protobuf.

3.  **Конфигурация через окружение**
    -   Никаких жестко закодированных адресов, портов или токенов в коде. Вся конфигурация передается через переменные окружения, что является стандартом для работы в Kubernetes.

4.  **Единообразие логирования**
    -   Все сервисы пишут структурированные (JSON) логи в `stdout`.
    -   Каждая запись лога содержит минимальный набор полей: `level` (info, warn, error), `timestamp`, `service_name`, `message`.

5.  **Отказоустойчивость по умолчанию**
    -   Любой сетевой вызов обрабатывается с учетом возможных сбоев: используются `retries` (повторные попытки) с `exponential backoff` и `timeouts` (таймауты).
    -   Сервисы спроектированы так, чтобы успешно стартовать и пытаться переподключиться к зависимостям (БД, NATS), даже если те временно недоступны.

---

## Часть 2: Асинхронное взаимодействие (NATS)

NATS — наша основная "нервная система" для асинхронного обмена сообщениями между сервисами.

1.  **Core NATS vs JetStream**
    -   **Core NATS ("Огонь-и-забыл"):** Используется для сообщений, потеря которых не критична. Например, отправка "живых" метрик или событий, которые не влияют на состояние системы.
    -   **JetStream (Гарантированная доставка):** Используется для всех критически важных бизнес-событий: создание игрока, обновление валюты, начало матча. Если сервис-получатель недоступен, JetStream сохранит сообщение и доставит его, когда сервис вернётся в строй.

2.  **Паттерны взаимодействия**
    -   **Publish/Subscribe:** Один сервис публикует событие (например, `player.event.player.created`), а один или несколько других сервисов на него реагируют. Это основной способ слабой связности (decoupling).
    -   **Request/Reply:** Используется, когда нужно получить ответ от другого сервиса асинхронно. Например, `Matchmaker` отправляет запрос `gamesession.command.room.find_available` и ждёт ответ с адресом свободного сервера.

3.  **Именование тем (Subjects)**
    -   Мы придерживаемся строгой схемы именования для всех тем:
        `{сервис}.{тип_события}.{объект}.{действие}`
    -   **Примеры:** `player.event.currency.updated`, `matchmaker.command.room.create`.

3.  **Контракты сообщений (Protobuf)**
    -   Все сообщения, передаваемые через NATS, **должны быть** сериализованы с помощью Protobuf. Использование сырого JSON запрещено.
    -   `.proto` файлы являются единым источником правды для контрактов и хранятся в отдельном `common-proto` репозитории.

---

## Часть 3: Admin API и Безопасность

Для управления системой каждый сервис, где это необходимо, предоставляет защищенные административные API-эндпоинты.

1.  **Аутентификация по API-ключу**
    -   Доступ ко всем admin-эндпоинтам осуществляется через единый HTTP-заголовок `admin-api-key`.
    -   Ключ представляет собой безопасный UUIDv4.

2.  **Централизованная валидация**
    -   `Auth Service` хранит таблицу `Admin` с email и хешированными API-ключами.
    -   Другие сервисы для проверки ключа отправляют запрос в `Auth Service` через NATS. Прямого доступа к таблице `Admin` у них нет.

3.  **Хранение и ротация ключей**
    -   В production-окружении API-ключи **обязательно** хранятся в Kubernetes Secrets или аналогичном безопасном хранилище.
    -   Должна быть предусмотрена процедура регулярной ротации (замены) ключей.

4.  **Дополнительные меры безопасности (Рекомендации)**
    -   **IP Whitelisting:** На уровне Ingress-контроллера (Nginx) доступ к admin-эндпоинтам (`/admin/*`) должен быть ограничен списком разрешенных IP-адресов.
    -   **Rate Limiting:** Для защиты от брутфорс-атак на эндпоинты должен быть настроен лимит запросов.

---

## Часть 4: Структура Монорепозитория

Проект организован как монорепозиторий для упрощения управления зависимостями и общим кодом.

1.  **Основные директории**
    -   `apps/`: Код каждого отдельного микросервиса.
    -   `libs/`: Переиспользуемые библиотеки, доступные для всех сервисов.
    -   `prisma/`: Схемы, миграции и сиды для всех баз данных.
    -   `docker/`: Dockerfiles и конфигурации для контейнеризации.
    -   `helm/`: Helm-чарты для развертывания в Kubernetes.
    -   `docs/`: Проектная и техническая документация.

2.  **Общие библиотеки (`libs/`)**
    -   `@lib/shared`: Содержит наиболее общий код: утилиты, базовые классы, конфигурацию, модули аутентификации, метрик и т.д.
    -   `@lib/contracts-*`: Отдельные библиотеки для каждого сервиса, определяющие его публичные контракты для взаимодействия через NATS (subjects, schemas). Это позволяет другим сервисам общаться с ним типобезопасно.

3.  **Path Aliases**
    -   В `tsconfig.json` настроены алиасы для чистоты импортов: `@app/*` для сервисов, `@lib/*` для библиотек. Это делает код более читаемым и упрощает рефакторинг.

---

## Часть 5: Архитектура Микросервиса (Clean Architecture + CQRS)

Каждый микросервис внутри `apps/` построен по единому шаблону, основанному на принципах Clean Architecture и CQRS.

1.  **Слои Архитектуры**
    -   **Domain:** Ядро сервиса. Содержит чистую бизнес-логику, доменные сущности и value-objects. **Не имеет зависимостей** от фреймворков или баз данных.
    -   **Application:** Оркестрирует бизнес-логику. Содержит CQRS-хэндлеры (use cases) и определяет "порты" (абстрактные интерфейсы для репозиториев, внешних сервисов).
    -   **Infrastructure:** Реализует "порты" из Application слоя. Здесь находится код для работы с PostgreSQL (через Prisma), Redis, внешними API.
    -   **Presentation:** Точка входа в сервис. Содержит HTTP-контроллеры и NATS-хэндлеры.

2.  **Правило Зависимостей**
    -   Зависимости направлены строго "внутрь": `Presentation -> Application -> Domain`. `Infrastructure` также зависит от `Application` и `Domain`.
    -   Это позволяет заменять детали реализации (например, сменить PostgreSQL на MongoDB), не затрагивая бизнес-логику.

3.  **CQRS (Command Query Responsibility Segregation)**
    -   **Commands:** Используются для всех операций, изменяющих состояние системы (создание, обновление, удаление).
    -   **Queries:** Используются для всех операций чтения данных.
    -   Такое разделение упрощает логику и позволяет оптимизировать чтение и запись независимо.

---

## Часть 6: Стиль Кода и Соглашения

1.  **Работа с Финансовыми Данными**
    -   Для представления денег, очков и любых других числовых значений, требующих точности, **обязательно** используется `BigNumber` (библиотека `bignumber.js`) во всех слоях, кроме `Infrastructure`.
    -   Тип `bigint` используется только на уровне ORM (Prisma) для маппинга в `BIGINT` в базе данных.
    -   В DTO (HTTP, NATS) все числовые идентификаторы и суммы передаются как `string` для предотвращения потери точности в JSON.

2.  **Валидация (Zod)**
    -   Все входящие данные (HTTP DTO, сообщения NATS) **обязательно** валидируются с помощью Zod-схем.
    -   Это обеспечивает строгую типизацию и защиту от некорректных данных на входе в систему.

3.  **Логирование**
    -   Используется структурированное (JSON) логирование.
    -   **Запрещено** логировать чувствительные данные (пароли, полные email, токены). Используйте маскирование.
    -   Бизнес-логика в `Domain` слое **не должна** содержать логов. Логирование — ответственность `Presentation`, `Application` и `Infrastructure` слоев.

4.  **Тестирование**
    -   Основной фокус — на тестировании **бизнес-логики** в `Domain` (Unit-тесты) и сценариев использования в `Application` (Unit-тесты с моками портов).
    -   `Infrastructure` слой покрывается интеграционными тестами с реальной (тестовой) базой данных.
    -   Качество тестов и покрытие ключевых сценариев важнее, чем абсолютный процент покрытия.

---

## Часть 7: Работа с Базой Данных (PostgreSQL + Prisma)

1.  **Принцип "Database per Service"**
    -   Каждый микросервис имеет **эксклюзивный доступ** к своей собственной базе данных.
    -   **Запрещено** напрямую обращаться к базе данных другого сервиса. Для получения данных из другого сервиса необходимо использовать NATS Request/Reply или подписываться на его события.

2.  **Управление Схемой и Миграциями**
    -   Все схемы и миграции для всех баз данных хранятся централизованно в директории `prisma/`.
    -   Для выполнения операций (миграция, генерация клиента) используются npm-скрипты, которые устанавливают переменную окружения `PRISMA_DB` и вызывают `prisma CLI`. Например: `npm run migrate:player:dev`.

3.  **Connection Pooling (PgBouncer)**
    -   Все подключения от сервисов к PostgreSQL в production-окружении **обязательно** должны проходить через `PgBouncer`. Это критически важно для предотвращения исчерпания лимита соединений на сервере БД.

4.  **Соглашения по Схеме**
    -   Имена таблиц и полей в БД именуются в `snake_case`. Для маппинга из `camelCase` (стиль Prisma) используются директивы `@@map` и `@map`.
    -   Все идентификаторы (`id`) используют тип `BigInt` в Prisma (`@db.BigInt`), что соответствует `BIGINT` в PostgreSQL.

---

## Часть 8: Локальная Разработка

1.  **Гибридный Подход**
    -   Инфраструктурные компоненты (PostgreSQL, NATS, Redis, PgBouncer) запускаются как Docker-контейнеры через `docker-compose up`.
    -   Микросервисы запускаются локально на хост-машине через npm-скрипты (`npm run start:player:debug`). Это позволяет использовать hot-reload для быстрой разработки.

2.  **Workflow Первого Запуска**
    -   `docker-compose up -d`: Запуск всей необходимой инфраструктуры.
    -   `npm install`: Установка зависимостей.
    -   `npm run generate:all`: Генерация всех Prisma-клиентов.
    -   `npm run migrate:all:dev`: Применение всех миграций к базам данных.
    -   `npm run seed:all`: Наполнение баз начальными данными.
    -   `npm run start:{service}:debug`: Запуск нужного сервиса в watch-режиме.

---

## Часть 9: Контейнеризация (Docker)

1.  **Multistage Builds**
    -   Для сборки Docker-образов **обязательно** используется двухэтапный (multistage) подход.
    -   **Этап `builder`:** Устанавливает все `devDependencies`, компилирует TypeScript, генерирует Prisma-клиенты.
    -   **Этап `production`:** Копирует только скомпилированный код (`dist/`), `node_modules` и `package.json` из `builder`. Это создает минималистичный и безопасный образ для production.

2.  **Автоматические миграции**
    -   Точка входа (`CMD`) в Docker-контейнере запускает `prisma migrate deploy` перед стартом основного приложения. Это гарантирует, что при развертывании новой версии кода схема БД будет автоматически обновлена.

---

## Часть 10: Развертывание (Kubernetes + Helm)

1.  **Umbrella Chart**
    -   Для управления всем стеком микросервисов используется "зонтичный" Helm-чарт. Он определяет каждый микросервис как зависимость, используя один и тот же переиспользуемый `sub-chart`.
    -   Это позволяет развертывать или обновлять всю систему одной командой.

2.  **Переиспользуемый Sub-chart**
    -   Создан универсальный `sub-chart` для всех бэкенд-сервисов. Он включает шаблоны для `Deployment`, `Service`, `ConfigMap`, `HPA` (Horizontal Pod Autoscaler).
    -   Конкретный сервис настраивается через передачу своих `values` в этот `sub-chart`.

3.  **Конфигурация по Окружениям**
    -   Конфигурация для разных сред (например, `dev` и `prod`) хранится в отдельных `values.yaml` файлах. Это позволяет гибко управлять ресурсами (CPU, memory), доменными именами и другими параметрами для каждой среды.

---

## Часть 11: CI/CD (GitHub Actions)

1.  **Сборка по Тегу**
    -   Процесс сборки Docker-образа запускается **автоматически** при отправке в репозиторий тега формата `apps/{service}@v{version}`.
    -   Это строго связывает версию кода в Git с версией артефакта (Docker-образа) в Container Registry.

2.  **Ручное Развертывание**
    -   Развертывание на `dev` и `prod` окружения запускается **вручную** через интерфейс GitHub Actions.
    -   Это обеспечивает дополнительный контроль и позволяет проводить тестирование на `dev` перед выкаткой в `prod`.

3.  **Управление Версиями**
    -   Для управления версиями и создания тегов используются npm-скрипты (`npm run {service}:patch`). Это стандартизирует процесс версионирования.

---

## Часть 12: Архитектура реалтайм сервисов (`websocket-service` + `gameplay-service`)

Реальное время реализуют два отдельных сервиса в пределах локальной зоны. Их архитектура принципиально отличается от обычных микросервисов.

1.  **Философия: Сервер — Источник Правды**
    -   Игровой сервер является **абсолютным источником правды** о состоянии игрового мира.
    -   Клиент отправляет только **намерения** (инпуты), а сервер решает, что из них выполнимо.
    -   Это ключевой принцип для защиты от читов и обеспечения честной игры.

2.  **Детерминированный Game Loop**
    -   Сервер запускает симуляцию мира с **фиксированным шагом времени** (например, 30 тиков в секунду).
    -   На каждом "тике" происходит: обработка входящих инпутов → обновление физики → сохранение состояния → рассылка клиентам.
    -   Рассылка состояния (`WORLD_STATE`) может происходить реже (например, 10 FPS) для экономии трафика.

3.  **Буферы Состояний**
    -   **InputBuffer:** `Map<playerId, RingBuffer<PlayerInput>>` — хранит последние N инпутов от каждого клиента.
    -   **StateBuffer:** `RingBuffer<WorldState>` — хранит последние N состояний мира.
    -   Кольцевой буфер (RingBuffer) — эффективная структура, где при заполнении самый старый элемент затирается новым.

4.  **Компенсация Лага (Lag Compensation)**
    -   Проблема: из-за сетевой задержки действие игрока (например, выстрел) приходит на сервер "из прошлого".
    -   Решение: сервер "отматывает" состояние мира назад на момент, когда игрок на самом деле совершил действие.
    -   **Алгоритм:**
        1.  Получаем действие с `client_time` и `ping`.
        2.  Вычисляем `rewind_time = client_time - (ping / 2)`.
        3.  Находим в `StateBuffer` два ближайших состояния к `rewind_time`.
        4.  Интерполируем позиции всех сущностей на момент `rewind_time`.
        5.  Выполняем проверку попадания в этом "призрачном" мире прошлого.

5.  **Обработка Ввода**
    -   **Движение:** Сервер получает `joystick_vector` и применяет velocity к персонажу, если `magnitude > MOVE_THRESHOLD`.
    -   **Прицеливание:** Если `magnitude > AIM_THRESHOLD`, обновляется направление взгляда (`rotation`).
    -   **Автоматическое оружие:** Клиент отправляет флаг `is_firing` на каждом тике. Сервер сам проверяет: есть ли патроны, прошёл ли кулдаун — и решает, произвести ли выстрел. Это защищает от манипуляций со скорострельностью.

6.  **Основные Компоненты**
    -   **`websocket-service`:** Единственная точка входа для WebSocket-соединений. Принимает сообщения от клиентов, валидирует JWT, проксирует сообщения в `gameplay-service` через NATS.
    -   **`gameplay-service`:** Запускает детерминированные симуляции (один инстанс на матч). Игровая логика спрятана внутри приватной сети кластера.
    -   **GameLoop:** Сердце `gameplay-service`. Работает с фиксированной частотой.
    -   **PhysicsSystem:** Детерминированная физическая симуляция (движение, коллизии).
    -   **LagCompensationSystem:** Использует буферы для "отмотки времени" при проверке попаданий.
    -   **AiDirectorProxy:** Прокси для взаимодействия с AI Director (отдельный сервис, управляющий сложностью и событиями).

---

## Часть 13: Публичный REST API для клиентов

Для взаимодействия клиента (игры) с мета-сервером используется публичный REST API.

1.  **Формат и Аутентификация**
    -   **Формат:** RESTful API с JSON.
    -   **Аутентификация:** JWT токены. Каждый запрос (кроме `/auth/*`) должен содержать заголовок `Authorization: Bearer <jwt_token>`.

2.  **Принцип "Single Source of Truth" для клиента**
    -   Любой запрос, изменяющий состояние игрока (`POST`, `PATCH`), в случае успеха **обязательно** возвращает полный, обновленный объект данных игрока (`UserData`).
    -   Это позволяет клиенту всегда иметь актуальное состояние, не делая дополнительных запросов.

3.  **Статическая конфигурация игры**
    -   Сервер предоставляет эндпоинт `/game/config` для получения всех статических игровых данных (стоимости, бонусы, параметры).
    -   Клиент **не должен** хранить эти данные локально в коде. Это позволяет балансировать игру без обновления клиента.

---

## Часть 14: Архитектура Аналитики

Для сбора и анализа игровых данных используется отдельная инфраструктура.

1.  **Инфраструктура: ClickHouse + Yandex DataLens**
    -   **Сбор данных:** Осуществляется через `Collector Service` — единую точку входа для всех игровых событий от клиента и сервера.
    -   **Хранение:** ClickHouse — OLAP база данных для аналитики.
    -   **Визуализация:** Yandex DataLens подключается к ClickHouse как к источнику данных.

2.  **Философия сбора событий**
    -   **Клиент — источник UX-событий:** Клиент отслеживает всё, что связано с взаимодействием пользователя с интерфейсом (нажатия кнопок, переходы по экранам, производительность).
    -   **Сервер — единственный источник правды:** Сервер отслеживает все **ключевые игровые и экономические события**. Результаты матчей, заработок/трата валюты, покупки — только сервер!
    -   **Правило:** Нельзя доверять клиенту в том, что касается денег или результатов матча.

3.  **Протокол отправки событий**
    -   Клиент и сервер формируют JSON-объект с именем события и его параметрами.
    -   Отправляют этот JSON по HTTP POST на эндпоинт `Collector Service`.
    -   `Collector Service` валидирует событие, обогащает его серверными данными (IP, timestamp) и складывает в очередь для пакетной вставки в ClickHouse.

---

## Часть 15: Оркестрация и Масштабирование Игровых Серверов

Архитектура игровых серверов разделена на два типа локаций для достижения низкого пинга и централизованного управления.

1.  **Центральная зона + Локальные зоны**
    -   **Центральная зона (Москва):** Сервисы, не требующие низкой задержки: Auth, Matchmaking, Meta API, а также основные базы данных (PostgreSQL, ClickHouse).
    -   **Локальные зоны (VPS в разных городах):** Только сервисы реального времени: `websocket-service`, `gameplay-service`, `local-orchestrator`. Находятся близко к игрокам.

2.  **`websocket-service` как прокси**
    -   `websocket-service` — единственная точка входа для всех WebSocket-соединений игроков в данном регионе.
    -   Он **не содержит** игровой логики, а только проксирует сообщения между клиентами и `gameplay-service` через NATS.
    -   **Преимущество:** Игровая логика спрятана внутри приватной сети кластера. Наружу смотрит только stateless `websocket-service`.

3.  **`gameplay-service` Heartbeat Статусы (через NATS)**
    -   Каждый инстанс `gameplay-service` раз в 5 секунд публикует своё состояние в тему `gameplay.service.heartbeat`.
    -   **Состав сообщения:** `server_id`, `region`, `city`, `public_ip`, `current_players`, `running_matches`, `max_matches`.
    -   `Matchmaking Service` подписан на эту тему и поддерживает актуальную таблицу всех доступных серверов и их загруженности.

4.  **Выбор сервера по GeoIP**
    -   `Matchmaking Service` получает IP-адрес клиента из запроса.
    -   С помощью GeoIP-базы определяет его местоположение.
    -   Выбирает лучший сервер, вычисляя "стоимость" подключения: `ping_to_server` (на основе расстояния) + `server_load_penalty`.

5.  **Одноразовые токены для подключения к матчу**
    -   При успешном матчмейкинге клиент получает: `match_id`, `websocket_url`, `token` (одноразовый).
    -   Первое сообщение клиента по WebSocket — аутентификация с `match_id` и `token`.
    -   Это защищает от подключения посторонних к матчу.

---

## Часть 16: Сетевой Протокол (WebSocket)

Игровой сервер использует WebSocket для real-time коммуникации с клиентами.

1.  **Жизненный цикл сессии**
    -   Клиент устанавливает WebSocket соединение.
    -   Клиент отправляет `AUTH` с токеном и `match_id`.
    -   Сервер проверяет токен и отправляет `JOIN_SUCCESS` с начальным состоянием мира.
    -   Начинается обмен `PLAYER_INPUT` (клиент → сервер) и `WORLD_STATE` (сервер → клиент).
    -   По завершении матча сервер отправляет `MATCH_END`.

2.  **Частоты обмена данными**
    -   **Клиент → Сервер (Input):** ~20 раз в секунду (каждые 50 мс). Это "снимок" состояния ввода игрока.
    -   **Сервер → Клиент (State Sync):** ~10 раз в секунду (каждые 100 мс). Отправляется `deltaState`, а не полное состояние.

3.  **Структура `PLAYER_INPUT`**
    -   `sequence_id`: Порядковый номер пакета для отслеживания потерь.
    -   `client_time`: Timestamp клиента для Lag Compensation.
    -   `ping`: RTT в миллисекундах, измеренный клиентом.
    -   `joystick_vector`: Текущее состояние виртуального джойстика.
    -   `actions`: **Очередь** мгновенных событий с прошлого тика (очищается после отправки).
    -   `states`: **Снимок** текущих зажатых состояний (например, `is_firing: true`).

---

## Часть 17: Сетевые Оптимизации

Для минимизации трафика и предотвращения читерства используются две ключевые техники.

1.  **Delta Compression (DeltaStateGenerator)**
    -   **Проблема:** Отправка полного состояния мира каждый тик — избыточна.
    -   **Решение:** Сервер для каждого клиента хранит `lastSentState`. Перед отправкой сравнивает `currentState` с `lastSentState` и создаёт легковесный `deltaState`, содержащий **только изменившиеся поля**.
    -   Клиент получает `deltaState` и "мерджит" его в локальное состояние.

2.  **Interest Management (InterestManager)**
    -   **Проблема:** Все клиенты получают одинаковый `WORLD_STATE`, включая данные об объектах вне их зоны видимости (лишний трафик + уязвимость для map-хаков).
    -   **Решение (Radius-Based):** Для каждого клиента сервер определяет его "зону интереса" (радиус `INTEREST_RADIUS`, ~2000 пикселей). В `deltaState` включаются только объекты внутри этого радиуса.
    -   **Глобальные события:** Сущности с флагом `isGlobal` (например, "база захвачена") игнорируют Interest Management и рассылаются всем.

3.  **Lag Compensation System**
    -   Использует `client_time` из `PLAYER_INPUT` для "отмотки" состояния мира и честной обработки попаданий для игроков с высоким пингом.

---

## Часть 18: Архитектура Баз Данных

Помимо принципа "Database per Service" (Часть 8), для высоконагруженных сценариев используется логическое и технологическое разделение.

1.  **Логическое Разделение PostgreSQL**
    -   Для распределения нагрузки данные разносятся по разным инстансам:
        -   **`Postgres_Users`:** Аккаунты, профили, социальный граф, баны. Нагрузка: в основном чтение.
        -   **`Postgres_Meta`:** Инвентарь, валюта, прогрессия, статистика. **Самая нагруженная база** — интенсивные чтение и запись.
        -   **`Postgres_Misc`:** Лидерборды, история матчей, данные гильдий. Нагрузка: смешанная.

2.  **ClickHouse для Time-Series Данных (Реплеи и Логи)**
    -   **Задача:** Хранение огромных объёмов событий (инпуты игроков, состояния мира, команды AI). Нагрузка **экстремально высокая на запись**.
    -   **Почему ClickHouse:** Колончатая СУБД, высочайшая скорость записи (миллионы записей/сек), сжатие в 3-5 раз эффективнее PostgreSQL, встроенный TTL для автоудаления старых данных.
    -   **TTL:** Данные хранятся 1 месяц, затем автоматически удаляются.

3.  **Паттерн Collector Service (Батчевая Запись)**
    -   **Проблема:** Прямая запись из каждого игрового сервера в БД создаёт огромную нагрузку.
    -   **Решение:** Игровые серверы отправляют события по лёгкому протоколу (UDP/HTTP) в промежуточный `Collector Service`. Он собирает события в пачки (например, 1000 событий или раз в секунду) и выполняет одну большую вставку в ClickHouse.
    -   Это **на порядки** эффективнее множества мелких записей.

---

## Часть 19: Anti-Cheat и Валидация Игровых Действий

Сервер является **единственным источником правды**. Любые данные от клиента — это не команда, а **предложение**, которое сервер обязан проверить.

1.  **Игровой Цикл (Game Loop)**
    -   Игровой цикл работает с фиксированной частотой (например, 30 тиков в секунду).
    -   Используется паттерн **Strategy** для возможности подмены логики под разные режимы игры.
    -   Основные системы, вызываемые в цикле: `InputValidator`, `PlayerCorrector`, `PhysicsSystem`, `WorldStateBuilder`.

2.  **Валидация Инпутов (`InputValidator`)**
    -   **Speed Hack:** Проверяем, что `расстояние_перемещения <= макс_скорость * dt * 1.2` (допуск на лаг).
    -   **Cooldowns:** Проверяем, что `время_с_последнего_действия >= cooldown_действия`.
    -   **Дальность атаки:** Проверяем, что цель находится в пределах досягаемости оружия.
    -   При систематических нарушениях (например, 3+ флага за секунду) отправляем сигнал в `AntiCheatService`.

3.  **Коррекция Состояния (`PlayerCorrector`)**
    -   Если инпут невалиден, позиция игрока на сервере **не меняется**.
    -   В следующем `WORLD_STATE` клиент получит серверную позицию, что вызовет эффект "rubber banding" (резинки).

4.  **Реплей-система как инструмент Anti-Cheat**
    -   Сохраненные в ClickHouse инпуты и состояния мира используются для пост-анализа.
    -   Оффлайн-инструмент позволяет визуализировать матчи и анализировать подозрительные паттерны (aimbot, speedhack).

---

## Часть 20: Защита от DDoS-атак

Защита от DDoS — многоуровневая задача, решаемая и на уровне инфраструктуры, и на уровне кода.

1.  **Уровень Инфраструктуры**
    -   99% защиты от объемных атак (L3/L4) происходит **до** достижения наших VPS.
    -   Используется встроенная защита облачного провайдера (Яндекс.Облако, AWS, GCP).
    -   Критичные эндпоинты (сайт, API матчмейкинга) могут проксироваться через Cloudflare.
    -   Базы данных находятся во **внутренней (приватной) сети**, недоступной извне.

2.  **Rate Limiting для REST API**
    -   **Обязательно** на всех публичных эндпоинтах (например, `express-rate-limit`).
    -   Стандартный лимит: ~100 запросов в минуту с одного IP на эндпоинт.
    -   На "тяжелые" запросы (история матчей) устанавливаются более строгие лимиты.

3.  **Защита WebSocket Gateway**
    -   **Токен аутентификации:** Подключение принимается **только** с валидным одноразовым токеном от `Matchmaking Service`.
    -   **Лимит на размер пакета:** Установить `maxPayload` для защиты от атак на исчерпание памяти.
    -   **Rate Limiting на уровне сообщений:** Если клиент шлет `PLAYER_INPUT` 2000 раз в секунду вместо 20, он отключается.

---

## Часть 21: Валидация Покупок (IAP)

**Никогда не доверяй клиенту в вопросах денег.** Чек покупки **обязательно** проверяется напрямую у Apple/Google.

1.  **Архитектура: Паттерн Strategy**
    -   `IAPValidator` — интерфейс с методом `validate(receiptData): ValidationResult`.
    -   `AppleValidator`, `GooglePlayValidator` — конкретные реализации для каждой платформы.
    -   Добавление нового провайдера (Steam) = добавление нового класса-валидатора.

2.  **Процесс Валидации**
    -   Клиент совершает покупку через нативный SDK и получает `receiptData`.
    -   Клиент отправляет `POST /payments/validate` с `{ platform, receiptData }`.
    -   Сервер выбирает нужный валидатор и проверяет чек у Apple/Google.
    -   При успехе: начисляет валюту, отправляет аналитику, возвращает обновленные данные.

3.  **Изоляция**
    -   Валидация покупок вынесена в **отдельный микросервис** для безопасности и изоляции.

---

## Часть 22: Мониторинг и Health Checks

### 22.1. Архитектура портов (обязательно для всех сервисов)

Каждый микросервис **обязан** предоставлять два HTTP-сервера на разных портах:

| Порт | Назначение | Доступ |
|------|------------|--------|
| **APP_PORT** (3001-3010) | Основной API сервиса | Через API Gateway |
| **MANAGEMENT_PORT** (9001-9010) | Health + Metrics | Только внутренняя сеть k8s |

**Пример конфигурации:**
```yaml
# config/player-service.yaml
service:
  name: player-service
  appPort: 3002          # Основной API
  managementPort: 9002   # Health + Metrics
```

**Реализация:**
```typescript
// src/main.ts
import express from 'express';
import { register } from 'prom-client';

const appServer = express();
const managementServer = express();

// Основной API (порт 3002)
appServer.use('/api/v1', apiRoutes);
appServer.listen(process.env.APP_PORT || 3002);

// Management (порт 9002)
managementServer.get('/ping', (req, res) => res.send('pong'));
managementServer.get('/health', healthCheckHandler);
managementServer.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});
managementServer.listen(process.env.MANAGEMENT_PORT || 9002);
```

**Таблица портов сервисов:**

| Сервис | APP_PORT | MANAGEMENT_PORT |
|--------|----------|-----------------|
| Auth Service | 3001 | 9001 |
| Player Service | 3002 | 9002 |
| Building Service | 3004 | 9004 |
| Combat Progress Service | 3005 | 9005 |
| Wallet Service | 3006 | 9006 |
| Matchmaking Service | 3007 | 9007 |
| Collector Service | 3008 | 9008 |
| Payment Service | 3009 | 9009 |
| Scheduler Service | 3010 | 9010 |

**Kubernetes Probes:**
```yaml
# deployment.yaml
livenessProbe:
  httpGet:
    path: /ping
    port: 9002        # Management port!
  initialDelaySeconds: 5
  periodSeconds: 10

readinessProbe:
  httpGet:
    path: /health
    port: 9002        # Management port!
  initialDelaySeconds: 10
  periodSeconds: 5
```

**Prometheus scrape config:**
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'microservices'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port]
        target_label: __address__
        replacement: ${1}:9002  # Management port
```

---

### 22.2. Три столпа наблюдаемости (Observability)

-   **Логи (Logs):** Что произошло? (Конкретные события)
-   **Метрики (Metrics):** Как система себя чувствует? (CPU, RAM, RPS)
-   **Трейсы (Traces):** Где была проблема? (Путь запроса через сервисы)

### 22.3. Централизованное Логирование (ELK / Loki)

-   Сервисы пишут JSON-логи в `stdout` — **никогда** в файлы.
-   Агент (Fluentd/Logstash) собирает `stdout` из всех контейнеров.
-   Логи индексируются в Elasticsearch/Loki и доступны через Kibana/Grafana.
-   **Best Practice:** Включать в лог контекст (`match_id`, `user_id`) для фильтрации:
    ```typescript
    logger.info({ message: 'Player connected', match_id: '...', user_id: '...' });
    ```

### 22.4. Метрики (Prometheus + Grafana)

-   Каждый сервис предоставляет эндпоинт `/metrics` через библиотеку `prom-client`.
-   Prometheus периодически опрашивает все `/metrics` и сохраняет в time-series БД.
-   Grafana визуализирует метрики в виде дашбордов.
-   **Alertmanager** отправляет алерты (Telegram, Slack) при выходе метрик за пределы нормы.

### 22.5. Health Checks: Разделение `/ping` и `/health`

1.  **`GET /ping`**
    -   Легковесная проверка: сервис запущен и отвечает.
    -   **Никаких** проверок зависимостей — просто `200 OK`.
    -   Используется: другими сервисами, Kubernetes `liveness probes`.

2.  **`GET /health`**
    -   Глубокая проверка: сервис **полностью работоспособен**.
    -   Проверяет все критические зависимости (БД, NATS, кэши).
    -   Ответ при успехе: `200 OK` с деталями:
        ```json
        { "status": "ok", "dependencies": { "postgres": "ok", "nats": "ok" } }
        ```
    -   Ответ при ошибке: `503 Service Unavailable` с деталями.
    -   Используется: Kubernetes `readiness probes` (убирает инстанс из балансировщика при сбое).
